---
title: "Testing COI Parameters"
author: "Aris Paschalidis"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: vignette
    fig_caption: yes
editor_options:
  chunk_output_type: console
---

```{r imports, echo=F, warning=F, message=F, results='hide'}
library(coiaf)
```

In this analysis file, we aim to understand the effect of varying parameters on
our COI framework. The parameters that we will examine are:

* COI_range: A number indicating the range of COIs to compare the simulated 
data to.
* method: The method to be employed. One of "end", "ideal", "overall".
* dist_method: The distance method used to determine the distance between the 
theoretical and simulated curves for the "overall" method. One of "abs_sum", 
"sum_abs", "squared", "KL".
* weighted: An indicator indicating whether to compute weighted distance.
* coverage: Coverage at each locus.
* alpha: Shape parameter of the symmetric Dirichlet prior on strain proportions.
* overdispersion: The extent to which counts are over-dispersed relative to the 
binomial distribution. Counts are Beta-binomially distributed, with the beta 
distribution having shape parameters $p/overdispersion$ and 
$(1-p)/overdispersion$.
* epsilon: The probability of a single read being miscalled as the other allele.
Applies in both directions.
* seq_error: The level of sequencing error that is assumed.

#### Setting our PLAF
```{r}
# set seed
set.seed(1)

# Define number of loci, and distribution of minor allele frequencies
L <- 1e3
p <- rbeta(L, 1, 5)
p[p > 0.5] <- 1 - p[p > 0.5]
```

## COI
We first want to understand for what range our model can accurately predict
the COI.

```{r}
tcoi <- coi_test(COI = 2:40, repetitions = 100,
                 PLAF = p, method = "overall", dist_method = "squared")
tcoi$error_bias
```

Based on the above set of runs, it appears that the algorithm performs well for
a very large range of COIs. We see that as the true COI increases, the error
and bias likewise increase. However, even at a COI of 40, the error and bias are
small, at roughly 4 and -4, respectively. It is interesting to note that our
bias is always negative. This indicates that the algorithm consistently 
underpredicts the COI.

The fact that our model struggles with larger COIs makes sense as at that point,
the theoretical COI curves are essentially identical. Although it is remarkable
that the algorithm performs this well at COIs that large, COIs of greater than 
20 are somehwat unrealistic. For the rest of this document, we only consider 
COIs of 2 till 20.

## COI Range
```{r}
tcoi_range <- coi_test(COI = 2:20, COI_range = 10:20, repetitions = 100,
                 PLAF = p, method = "overall", dist_method = "squared")
tcoi_range$error_bias
```

As expected, the lower the COI range, the lower the error and bias. This makes
intuitive sense as the COI_range allows us to determine how many theoretical
COI values we will test to see which our simulated data is closest to. The fact
that our model works well with large `COI_range` values is promising and implies
that our model can accurately compute the COI.

## Method
```{r}
tmethod <- coi_test(COI = 2:20, PLAF = p, dist_method = "squared", 
                 repetitions = 100)
tmethod$error_bias
```


## Distance Method
```{r}
test <- coi_test(COI = 2:20, PLAF = p, method = "overall", repetitions = 100)
test$error_bias
```

## Weighted
```{r}
test <- coi_test(COI = 2:20, weighted = c(FALSE, TRUE),
                 PLAF = p, method = "overall", repetitions = 100)
test$error_bias
```

## Coverage
```{r}
test <- coi_test(COI = 2:20, coverage = c(25, 50, 100, 200, 400), 
                 repetitions = 100, PLAF = p,
                 method = "overall", dist_method = "squared")
test$error_bias
```

## Alpha
```{r}
test <- coi_test(COI = 2:20, alpha = 1:10, 
                 repetitions = 100, PLAF = p, 
                 method = "overall", dist_method = "squared")
test$error_bias
```

## Overdispersion
```{r}
test <- coi_test(COI = 2:20, overdispersion = seq(0, 0.1, 0.05), 
                 repetitions = 100, PLAF = p, 
                 method = "overall", dist_method = "squared")
test$error_bias
```

When you introduce overdispersion, models perform poorly. If you introduce 0.1, 
models do not perform terribly. What is the cutoff here? How much overdispersion
can we introduce?

## Epsilon
```{r}
test <- coi_test(COI = 2:20, epsilon = seq(0, 1, 0.1), 
                 repetitions = 100, PLAF = p, 
                 method = "overall", dist_method = "squared")
test$error_bias
```

## Sequencing Error
```{r}
test <- coi_test(COI = 2:20, seq_error = seq(0, 0.1, 0.02), 
                 repetitions = 100, PLAF = p, 
                 method = "overall", dist_method = "squared")
test$error_bias
```
